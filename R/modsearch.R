##' @title Generate a list of \code{\linkS4class{ROCit}} objects or \code{\linkS4class{RMSEit}} for different datasets
##' @param fit a \code{\linkS4class{train}} object generated by \code{\link{ROCtest}} or \code{\link{RMSEtest}}
##' @param datatype a named character representing the accuracy object be built on either "train" or "test" data, 
##' user can include both
##' @param testdata a list of length two containing a named slot for the matrix of predictors 
##' (pred) and the vector of classes (class)
##' @param modelKeep a logical indicating whether the original model object should be stored, default is FALSE
##' @param ... additional arguments passed to \code{\link{modSearch}}
##' @return a list with the following:
##' \itemize{
##' \item{method - the \code{\link{train}} method used to fit the model}
##' \item{summaryTr - the \code{\linkS4class{ROCit}} or \code{\linkS4class{RMSEit}} for the training data}
##' \item{summaryTe - the \code{\linkS4class{ROCit}} \code{\linkS4class{RMSEit}} for the test data}
##' \item{time - the time reported for the model to run, taken from the \code{\link{train}} object if available}
##' \item{metricSD - the standard deviation of the metric for the best model reported by train}
##' }
##' @note The values presented are for the optimal threshold as computed by the \code{\link{roc}} function for ROC objects.
##' @export
modAcc <- function(fit, datatype = c("test", "train"), testdata, modelKeep = FALSE, ...){
  if (missing(modelKeep)){
    modelKeep <- FALSE
  }
  if(!exists("metric", where = fit)){
    if(class(fit)[1] == "glm" | class(fit[1]) == "lm"){
      if(length(unique(fit$y)) == 2){
        fit$metric <- "ROC"
      } else{
        fit$metric <- "RMSE"
      }
    }
  }
  if(missing(datatype)){
    datatype <- "train"
    message("Training data only being used. Specify datatype = 'test' and give test data to testdata.")
  }
  if("test" %in% datatype & missing(testdata)){
    stop("Please provide testdata")
  }
  if(fit$metric == "ROC"){
    SD <- fit$results$ROCSD[fit$results$ROC == max(fit$results$ROC)]
    if (length(datatype) > 1){
      train <- ROCtest(fit, ...)
      test <-  ROCtest(fit, testdata=list(preds = testdata$preds, 
                                          class = testdata$class), ...)    
    } else if(length(datatype) < 2 & datatype=="test"){
      test <- ROCtest(fit, testdata=list(preds = testdata$preds, 
                                         class = testdata$class), ...)
      train <- NULL
    } else if (length(datatype) < 2 & datatype=="train"){
      train <- ROCtest(fit, ...)
      test <- NULL
    }
  } else if(fit$metric == "RMSE"){
    SD <- fit$results$"RMSE SD"[fit$results$RMSE == min(fit$results$RMSE)]
    if (length(datatype) > 1){
      train <- RMSEtest(fit)
      test <-  RMSEtest(fit, testdata=list(preds = testdata$preds, 
                                             class = testdata$class))    
    } else if(length(datatype) < 2 & datatype=="test"){
      test <- RMSEtest(fit, testdata=list(preds = testdata$preds, 
                                            class = testdata$class))
      train <- NULL
    } else if (length(datatype) < 2 & datatype=="train"){
      train <- RMSEtest(fit)
      test <- NULL
    }
  }
  if(modelKeep == TRUE){
    return(list(model=fit, summaryTr = train, summaryTe = test, method=fit$method, 
                time = fit$times$everything[3], metric = fit$metric, metricSD = SD))
  } else if(modelKeep == FALSE){
    return(list(method=fit$method, summaryTr = train, summaryTe = test, 
                time = fit$times$everything[3], metric = fit$metric, metricSD = SD))
  }
}

##' @title Generate a dataframe from \code{\link{modAcc}} lists
##' @description Used for generating the data to make good looking RMSE plots for 
##' training and test data simultaneously
##' @param mod a list resulting from a call to \code{\link{modAcc}}
##' @return a \code{\link{data.frame}} with the following columns:
##' \itemize{
##' \item{RMSE - the RMSE for the model on the test or training data}
##' \item{RMSEsd - the standard deviation of the RMSE reported by training with caret}
##' \item{Rsquare - The R squared reported for the best-fit in caret}
##' \item{RsquareSD - the model method}
##' \item{method - the area under the curve}
##' \item{grp - the area under the curve}
##' \item{elapsedTime - the time reported for the model to run}
##' }
##' @note The measures come from the \code{\linkS4class{RMSEit}} object
##' @export 
dfExtractRMSE <- function(mod){
  if(class(mod$summaryTr) != "NULL"){
    newdatTr <- data.frame(RMSE= mod$summaryTr@RMSE, 
                           RMSEsd = max(mod$summaryTr@bestFit$RMSESD), 
                           Rsquare = max(mod$summaryTr@bestFit$Rsquared), 
                           RsquareSD = max(mod$summaryTr@bestFit$RsquaredSD), 
                           method = mod$method, 
                           grp = "train", 
                           elapsedTime = ifelse(is.null(mod$time), NA, mod$time), 
                           check.rows = FALSE, row.names = NULL)
  }
  if(class(mod$summaryTe) != "NULL"){
    newdatTe <- data.frame(RMSE= mod$summaryTe@RMSE, 
                           RMSEsd = max(mod$summaryTe@bestFit$RMSESD), 
                           Rsquare = max(mod$summaryTe@bestFit$Rsquared), 
                           RsquareSD = max(mod$summaryTe@bestFit$RsquaredSD), 
                           method = mod$method, 
                           grp = "test", 
                           elapsedTime = ifelse(is.null(mod$time), NA, mod$time), 
                           check.rows = FALSE, row.names = NULL)
  }
  if(class(mod$summaryTr) != "NULL" & class(mod$summaryTe) != "NULL"){
    tmp <- rbind(newdatTr, newdatTe)
  } else if(class(mod$summaryTr) != "NULL"){
    tmp <- newdatTr
  } else if(class(mod$summaryTe) != "NULL"){
    tmp <- newdatTe
  }
  tmp$RMSE <- as.numeric(tmp$RMSE)
  tmp$RMSEsd <- as.numeric(tmp$RMSEsd)
  tmp$Rsquare <- as.numeric(tmp$Rsquare)
  tmp$RsquareSD <- as.numeric(tmp$RsquareSD)
  tmp$method <- as.character(tmp$method)
  tmp$grp <- as.character(tmp$grp)
  tmp$elapsedTime <- as.numeric(tmp$elapsedTime)
  return(tmp)
}

##'@title Generate a dataframe from \code{\link{modAcc}} lists
##'@description Generic function to extract either ROC data or RMSE data from models
##'@param mod a list resulting from a call to \code{\link{modAcc}}
##'@return a data.frame with columns relating to RMSE or ROC fits
##'@note See dfExtractRMSE or dfExtractROC for details
##'@export
dfExtract <- function(mod){
  if(mod$metric == "ROC"){
    tmp <- dfExtractROC(mod)
  } else if(mod$metric == "RMSE"){
    tmp <- dfExtractRMSE(mod)
  }
  return(tmp)
}


##' @title Generate a dataframe from \code{\link{modAcc}} lists
##' @description Used for generating the data to make good looking ROC curves of 
##' training and test data.
##' @param mod a list resulting from a call to \code{\link{modAcc}}
##' @return a \code{\link{data.frame}} with the following columns:
##' \itemize{
##' \item{sens - the sensitivities of the model at various thresholds}
##' \item{spec - the specificities of the model at various thresholds}
##' \item{grp - whether the model is using training or test data}
##' \item{method - the model method}
##' \item{auc - the area under the curve}
##' \item{aucSD - the standard deviation of the AUC for the best model reported by train}
##' \item{elapsedTime - the time reported for the model to run}
##' }
##' @note The sensitivities and specificities come from the \code{\link{roc}} object stored in the 
##' \code{\linkS4class{ROCit}} object
##' @export
dfExtractROC <- function(mod){
  if(class(mod$summaryTr) != "NULL"){
      newdatB <- data.frame(sens = smooth(mod$summaryTr@rocobj)$sensitivities, 
                            spec = smooth(mod$summaryTr@rocobj)$specificities, 
                            grp="train", 
                            auc = mod$summaryTr@auc,
                            method = mod$method, 
                            aucSD = mod$metricSD,
                            elapsedTime = ifelse(is.null(mod$time), NA, mod$time), 
                            check.rows=FALSE, 
                            row.names=NULL)
      }
    if(class(mod$summaryTe) != "NULL"){
      newdatA <- data.frame(sens = smooth(mod$summaryTe@rocobj)$sensitivities, 
                            spec = smooth(mod$summaryTe@rocobj)$specificities, 
                            grp="test",
                            auc  = mod$summaryTe@auc,
                            aucSD = mod$metricSD,
                            method = mod$method, 
                            elapsedTime =ifelse(is.null(mod$time), NA, mod$time), 
                            check.rows=FALSE, 
                            row.names=NULL)
      }    
    if(class(mod$summaryTr) != "NULL" & class(mod$summaryTe) != "NULL"){
      tmp <- rbind(newdatA, newdatB)
    } else if(class(mod$summaryTr) != "NULL"){
      tmp <- newdatB
    } else if(class(mod$summaryTe) != "NULL"){
      tmp <- newdatA
    }
    tmp$sens <- as.numeric(tmp$sens)
    tmp$spec <- as.numeric(tmp$spec)
    tmp$auc <- as.numeric(tmp$auc)
    tmp$method <- as.character(tmp$method)
    tmp$auc <- as.numeric(tmp$auc)
    tmp$grp <- as.character(tmp$grp)
    tmp$aucSD <- as.numeric(tmp$aucSD)
    tmp$elapsedTime <- as.numeric(tmp$elapsedTime)
    tmp <- tmp[, c("sens", "spec", "auc", "aucSD","method", "grp", "elapsedTime")]
    return(tmp)
}

##' @title Train a model and store \code{\linkS4class{ROCit}} tests on different datasets
##' @description This function wraps the \code{train} function in the \code{caret} package with model accuracy reports. 
##' It also allows for errors in fitting models to be caught to make it easier to use in a loop. 
##' @param method a a string specifying which classification or regression model to use. Possible values are found using \code{names(getModelInfo())}. See http://caret.r-forge.r-project.org/bytag.html. A list of funciotns can also be passed for a custom model function. See http://caret.r-forge.r-project.org/custom_models.html for details
##' @param datatype a named character representing the accuracy object be built on either "train" or "test" data, 
##' user can include both
##' @param traindata a list of length two containing a named slot for the matrix of predictors 
##' (pred) and the vector of classes (class)
##' @param testdata a list of length two containing a named slot for the matrix of predictors 
##' (pred) and the vector of classes (class)
##' @param modelKeep a logical indicating whether the original model object should be stored
##' @param length an integer denoting the number of levels of each tuning parameter 
##' that should be generated to be passed to \code{tuneLength} in the \code{train} call
##' @param fitControl an object generated by \code{trainControl} to control the
##'  behavior of \code{train}. If none is given a default is selected.
##' @param metric a character string passed to \code{train}. a string that specifies what 
##' summary metric will be used to select the optimal model. By default, possible 
##' values are "RMSE" and "Rsquared" for regression and "Accuracy" and "Kappa" for 
##' classification. If custom performance metrics are used (via the \code{summaryFunction}
##' argument in trainControl, the value of metric should match one of the arguments. 
##' If it does not, a warning is issued and the first metric given by the 
##' summaryFunction is used. 
##' @param cores An integer representing the number of cores to use on Windows. If not on windows, a warning is issued. 
##' @return A character string with an error if unsuccessful. The result of the \code{modAcc} call if successful: 
##' \itemize{
##' \item{method - the \code{\link{train}} method used to fit the model}
##' \item{summaryTr - the \code{\linkS4class{ROCit}} for the training data}
##' \item{summaryTe - the \code{\linkS4class{ROCit}} for the test data}
##' \item{time - the time reported for the model to run, taken from the \code{\link{train}} object if available}
##' }
##' @note The values presented are for the optimal threshold as computed by the \code{\link{roc}} function.
##' For some model types linear combos of predictors may be omitted.
##' @export
modTest <- function(method, datatype=c("train", "test"), traindata, testdata, 
                      modelKeep=FALSE, length, fitControl = NULL, 
                    metric = "ROC", cores = NULL){
  
    args <- as.list(substitute(list(...)))
    if("omit" %in% names(args)){
      stop("Cannot omit an index of variables. Instead see caret:::findLinearCombos")
    }
  # Let's dump out some defaults
  # Set up cores for Windows
  if(!missing(cores)){
    myOS <- Sys.info()['sysname']
    if(myOS!="Windows"){  
      warning("Only declare cores on Windows machines. On Linux 
                             you can declare parallel outside of the modTest 
                             or modSearch call.")
    } else {
      myclus <- makeCluster(cores)
      registerDoParallel(myclus)
    } 
  }
  datD <- c('rda', 'lda2', 'hda', 'mda', 'mlp', 'mlpWeightDecay', 
            'rbf', 'rpart2', 'C5.0Rules', 'pda2', 'rda', 'glm',
            'treebag', 'rf', 'plr', 'lda', 'xyf', 'sddaLDA', 'sddaQDA', 
            'LogitBoost', 'C5.0', 'bag', 'C5.0Tree')
  if(method %in% datD & metric == "ROC"){
    omit <- findLinearCombos(traindata$preds)$remove
    cols <- 1:ncol(traindata$preds)
    keep <- cols[!cols %in% omit]
  } else if(metric == "RMSE"){
    omit <- findLinearCombos(traindata$preds)$remove
    cols <- 1:ncol(traindata$preds)
    keep <- cols[!cols %in% omit]
  } else{
    keep <-  1:ncol(traindata$preds)
  }
    fit <- tryCatch({
      train(traindata$preds[, keep], traindata$class,
            method=method,
            trControl=fitControl,
            tuneLength = length, metric= metric)}, error = function(e) 
              message(paste0("Model failed to run: ", method)))
  # multicore
  if(!missing(cores)){
    if(myOS == "Windows"){
      try(stopCluster(myclus))
      try(stopImplicitCluster())
    }
  }
  if(class(fit) == "character"){
    message(paste0("Model failed to run: ", method))
    
  } else if(class(fit) == "train"){
      fitSum <- modAcc(fit, datatype = datatype, 
                     testdata=list(preds = testdata$preds[, keep], 
                                   class = testdata$class ), 
                     modelKeep = modelKeep)
    } 
  return(fitSum)
}

##' @title Generate an empty dataframe to match \code{\link{modAcc}} lists
##' @description Used for generating the data to make good looking ROC curves of 
##' training and test data.
##' @param methods a list of \code{train} method names to generate the dataframe for
##' @return a \code{\link{data.frame}} with the following columns:
##' \itemize{
##' \item{sens - the sensitivities of the model at various thresholds}
##' \item{spec - the specificities of the model at various thresholds}
##' \item{auc - the area under the curve}
##' \item{auc - the standard deviation of the AUC for the best method reported by train}
##' \item{method - the model method}
##' \item{grp - whether the model is using training or test data}
##' \item{elapsedTime - the time reported for the model to run}
##' }
##' @note The sensitivities and specificities come from the \code{\link{roc}} object stored in the 
##' \code{\linkS4class{ROCit}} object
buildROCcurveFrame <- function(methods){
  ModelFits <- expand.grid(sens = NA, spec = NA,  auc = NA, aucSD = NA,
                           method = rep(methods, each = 1028), grp = NA,
                           elapsedTime = NA)
  
  # Class variables correctly to avoid errors
  ModelFits$grp <- as.character(ModelFits$grp)
  ModelFits$method <- as.character(ModelFits$method)
  ModelFits$sens <- as.numeric(ModelFits$sens)
  ModelFits$spec <- as.numeric(ModelFits$spec)
  ModelFits$auc <- as.numeric(ModelFits$auc)
  ModelFits$aucSD <- as.numeric(ModelFits$aucSD)
  return(ModelFits)
  
}

##' @title Generate an empty dataframe to match \code{\link{modAcc}} lists
##' @description Used for generating the data to make comparative RMSE plots.
##' @param methods a list of \code{train} method names to generate the dataframe for
##' @return a \code{\link{data.frame}} with the following columns:
##' \itemize{
##' \item{RMSE - the RMSE for the model on the test or training data}
##' \item{RMSEsd - the standard deviation of the RMSE reported by training with caret}
##' \item{Rsquare - The R squared reported for the best-fit in caret}
##' \item{RsquareSD - the model method}
##' \item{method - the area under the curve}
##' \item{grp - the area under the curve}
##' \item{elapsedTime - the time reported for the model to run}
##' }
##' @note The measures come from the \code{\linkS4class{RMSEit}} object
buildRMSEFrame <- function(methods){
  ModelFits <- expand.grid(RMSE = NA, RMSEsd = NA, Rsquare = NA, RsquareSD = NA, 
                           method = rep(methods, each = 1), grp = NA, 
                           elapsedTime = NA)
  # Class variables correctly to avoid errors
  ModelFits$grp <- as.character(ModelFits$grp)
  ModelFits$method <- as.character(ModelFits$method)
  ModelFits$RMSE <- as.numeric(ModelFits$RMSE)
  ModelFits$RMSEsd <- as.numeric(ModelFits$RMSEsd)
  ModelFits$Rsquare <- as.numeric(ModelFits$Rsquare)
  ModelFits$RsquareSD <- as.numeric(ModelFits$RsquareSD)
  return(ModelFits)
  
}


##' @title Generate an empty dataframe to match \code{\link{modAcc}} lists
##' @description Used for generating the data to make good looking ROC curves of 
##' training and test data.
##' @param methods a list of \code{train} method names to generate the dataframe for
##' @param ... additional arguments passed to \code{\link{modTest}}
##' @return a \code{\link{data.frame}} with the following columns:
##' \itemize{
##' \item{sens - the sensitivities of the model at various thresholds}
##' \item{spec - the specificities of the model at various thresholds}
##' \item{grp - whether the model is using training or test data}
##' \item{auc - the area under the curve}
##' \item{method - the model method}
##' \item{elapsedTime - the time reported for the model to run}
##' }
##' @note The sensitivities and specificities come from the \code{\link{roc}} object stored in the 
##' \code{\linkS4class{ROCit}} object
##' @export
##' 
modSearch <- function(methods, ...){
  args <- as.list(substitute(list(...)))[-1L]
  metric <- args$metric
  datatype <- args$datatype
  if(metric == "ROC"){
    if(length(datatype) > 1){
      ModelFits <-rbind(buildROCcurveFrame(methods), buildROCcurveFrame(methods))
    } else{
      ModelFits <- buildROCcurveFrame(methods)
    }
  } else if(metric == "RMSE"){
    if(length(datatype) > 1){
      ModelFits <- rbind(buildRMSEFrame(methods), buildRMSEFrame(methods))
    } else {
      ModelFits <- buildRMSEFrame(methods)
    }
    
  } else{
    stop("No custom performance frame defined for metric")
  }
  pb <- txtProgressBar(min = 0, max = length(methods), style = 3)
  for(i in methods){
    p <- match(i, methods)
    z<-list(method = i)
    z<-c(z,args)
    fit <- try(do.call(modTest, z), silent = TRUE)
    tmp <- tryCatch(dfExtract(fit), error = function(e) "No Model Ran")
    #
    if(class(tmp) == "data.frame"){
      ModelFits[ModelFits$method == i,] <- tmp[tmp$method == i,]
    } else{
      ModelFits <- ModelFits
      message(paste(tmp, "failure for model type:", i, sep=" "))
    }
      setTxtProgressBar(pb, p)      
  }
  ModelFits <- ModelFits[!duplicated(ModelFits),] # drop duplicates
  return(ModelFits)
}

# old block for timeout
# 
# if(!missing(timeout)){
#   timeout <- timeout
#   fit <- tryCatch({
#     evalWithTimeout({
#       do.call(modTest, z);
#     }, timeout = timeout, elapsed = timeout, onTimeout = "warning")},
#     TimeoutException = function(ex) {
#       print("Timeout. Skip")
#     }, error = function(e) {paste0("Failure of model: ", i, 
#                                    "\n", " For: ",e)})
#   
# } else if(missing(timeout)){
