<!--
%\VignetteEngine{knitr}
%\VignetteIndexEntry{Simulated Data}
-->

```{r, echo = FALSE, message = FALSE}
knitr::opts_chunk$set(
  comment = "#>",
  error = FALSE,
  tidy = FALSE
)

```

# Introduction

The `EWStools` package is designed to automate many of the model building and 
checking tests that are common in predictive analytics, specifically for the 
education domain. We will focus on this suite of tools around the ROC classification 
procedure. To do this, let's work with a simple simulated dataset we can 
construct using the `twoClassSim` function in the `caret` package:

```{r builddata}
set.seed(442)
library(caret)
train <- twoClassSim(n = 1000, intercept = -8, linearVars = 3, 
                        noiseVars = 10, corrVars = 4, corrValue = 0.6)
test <- twoClassSim(n = 1500, intercept = -7, linearVars = 3, 
                       noiseVars = 10, corrVars = 4, corrValue = 0.6)

```

Let's see what this produces

```{r inspectdata}
head(train[, c(1:5, 20:23)])
table(train$Class)
```

Our training data has an imbalanced class structure and 22 predictors that are scaled 
and centered.

A key thing to note is that the `train` and `test` data have the exact same 
variable names and scales:

```{r compare}
names(train)
names(test)
```

Now let's build a model:

```{r buildexamplemodel}
ctrl <- trainControl(method = "cv", classProbs = TRUE, 
                     summaryFunction = twoClassSummary)

fullModel <- train(Class ~ ., data = train, 
                   method = "knn", 
                   preProc = c("center", "scale"), 
                   tuneLength = 8, 
                   metric = "ROC", 
                   trControl = ctrl)

fullModel

```

The standard output from the `caret` object is nice, but we want a more flexible 
and robust classification testing suite built around the ROC specifically. 

# Build and compare models 

# Conveniently Compare Accuracy Between Train and Test Data

```{r roctest1}
test1 <- ROCtest(fullModel)
print(test1)
```

This tells us how the model fits the training data in a convenient and easy to 
interpret fashion. We get a sense of the area under the curve. We can also use 
the same function to evaluate performance on the test data using a named list of 
new data:

```{r roctesttestdata}
test2 <- ROCtest(fullModel, testdata = list(preds = test[, -23], 
                                            class = test[, "Class"]))

print(test2)
```

As we can see, the model performs less effectively on the test data. But, what 
if we want to compare many models?


# Model Accuracy Profiles

```{r modacc}
test3 <- modAcc(fullModel, datatype = c("train","test"), 
                testdata = list(preds = test[, 1:22], 
                                                     class = test[, 23]))

summary(dfExtract(test3))
```


# Compare GLMs

Sometimes we want to compare more theoretically driven generalized linear models to 
the machine learning algorithms fit with the `caret` package. While the `caret` package 
can certainly produce models using a glm, we can also compare `glm` objects directly. 


```{r glmexamp}
glmModel <- glm(Class ~ . , data=train, family = binomial)
testGLM <- ROCtest(glmModel)
```

```{r glmsummary}
print(testGLM)
```


And, we can use this to automate our process of inspecting the performance on 
test data as well:

```{r glmtestdata}
testGLM2 <- ROCtest(glmModel, testdata = list(preds = test[, -23], 
                                            class = test[, "Class"]))

print(testGLM2)
```

And, we can extract results and generate dataframes of the performance profiles of 
`glm` objects just as we can with `train` objects:

```{r glmmodacc}
testGLM3 <- modAcc(glmModel, datatype =c("train", "test"), testdata = list(preds = test[, 1:22], 
                                                     class = test[, 23]))

summary(dfExtract(testGLM3))
```


